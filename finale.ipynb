{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classifier for Uppercase Letters of the English Alphabet\n",
    "\n",
    "## Introduction\n",
    "In this project, we will build a classifier that can distinguish between scanned images of uppercase letters of the English alphabet. The dataset consists of a large number of rectangular pixels in black and white that refer to the 26 uppercase letters of the alphabet. The character images were based on 20 different fonts and each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli. Each stimulus was converted into 16 numerical primitive attributes (statistical moments and edge counts), which were then scaled to fit a range of integer values from 0 to 15.\n",
    "\n",
    "We will use various machine learning techniques, including a Multi-Layer Perceptron (MLP) neural network and a Radial Basis Function (RBF), to train our classifier. We will also use optimization routines from the scipy.optimize library to determine the optimal parameters for our models.\n",
    "\n",
    "Before we begin, let us import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from scipy.optimize import minimize\n",
    "import cvxopt.solvers\n",
    "from cvxopt import matrix, solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "In order to train our classifier, we first need to prepare the data. This entails the following steps:\n",
    "- Loading the data from a file\n",
    "- Splitting the data into input features x and output labels y\n",
    "- Converting the data into a format that is compatible with our machine learning models\n",
    "\n",
    "We define a function get_data that takes a filename as a parameter and returns the input features x and output labels y as numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.read()\n",
    "    data = data.split('\\n')\n",
    "    data = data[1:-1]\n",
    "    array = [data[i].split(',') for i in range(len(data))]\n",
    "    array = np.array(array)\n",
    "    x = array[:, 1:]\n",
    "    x = x.astype(int)\n",
    "    y = array[:, 0]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the binary classification task, we need to transform the output labels y into binary values. This is a function two_class that takes the output labels y as input and returns a binary version of y, where the value is 1 if the label is 'P' and 0 otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_class(y):\n",
    "    y = np.where(y == 'P', 1, 0)\n",
    "    return y.astype(int).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting\n",
    "\n",
    "One of the essential steps in any machine learning project is to split the data into a training set and a test set. The training set is used to fit our classifier to the data, while the test set is used to measure how well our classifier generalizes to new and unseen data. A common practice is to use a fixed ratio of 80% for the training set and 20% for the test set. The following function split_data takes as input the features x, the labels y, and the train ratio as parameters and returns the split data as numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x, y, train_ratio):\n",
    "    train_size = int(len(x) * train_ratio)\n",
    "    x_train = x[:train_size]\n",
    "    x_test = x[train_size:]\n",
    "    y_train = y[:train_size]\n",
    "    y_test = y[train_size:]\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to optimize the hyperparameters of our models, we will apply a k-fold cross-validation technique. This consists of splitting the training set into k equal-sized folds, using one fold as a validation set and the remaining ones as a training set, and repeating this procedure for each fold. This allows us to obtain an average validation error for each hyperparameter configuration and select the optimal one. The following function k_fold takes as input the input features x, the output labels y, and the number of folds k and returns a list of x folds and a list of y folds as numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(x, y, k):\n",
    "    if k == 0:\n",
    "        return x, y\n",
    "    x_folds = np.array_split(x, k)\n",
    "    y_folds = np.array_split(y, k)\n",
    "    return x_folds, y_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_data('data.txt')\n",
    "y = two_class(y)\n",
    "rho = 10**(-4) # regularization term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - MLP\n",
    "Part 1 - MLP\n",
    "In this section, we will implement a Multi-Layer Perceptron (MLP) network, which is a type of artificial neural network that can learn complex nonlinear functions. We will train the MLP network using the regularized binary cross-entropy loss function, which is suitable for binary classification problems.\n",
    "\n",
    "The MLP network consists of the following components:\n",
    "- An input layer that receives a feature vector x as input and passes it to the next layer.\n",
    "- One or more hidden layers that perform nonlinear transformations on the input using weighted connections and activation functions. We will use the hyperbolic tangent function as the activation function, which is defined as:\n",
    "    $$g(t):=\\tanh (t)=\\frac{e^{2 \\sigma t}-1}{e^{2 \\sigma t}+1}$$\n",
    "- An output layer that produces an output vector y_pred, which represents the predicted probability of each class. We will use the Softmax function as the output function, which is defined as:\n",
    "    $$\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}$$\n",
    "    where n is the number of classes.\n",
    "\n",
    "The regularized binary cross-entropy loss function is defined as:\n",
    "     $$E(\\omega ; \\pi)=-\\frac{1}{P} \\sum_{i=1}^P\\left[y_i \\ln \\left(p_i\\right)+\\left(1-y_i\\right) \\ln \\left(1-p_i\\right)\\right]+\\rho\\|\\omega\\|^2$$\n",
    "    where P is the number of samples, y and p are the true and predicted labels, respectively, w is the weight vector, and rho is the regularization parameter. The loss function quantifies the discrepancy between the true and predicted labels by taking the negative logarithm of the probabilities. The lower the probability of the correct class, the higher the loss. The loss function also includes a regularization term that penalizes large weights and prevents overfitting.\n",
    "\n",
    "The hyperparameters for our MLP network are:\n",
    "- The number H of hidden layers (max. 4)\n",
    "- The number of neurons N in each hidden layer\n",
    "- The spread sigma of the activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(t, sigma): \n",
    "    return np.tanh(t*sigma)\n",
    "\n",
    "def softmax(v):\n",
    "    ones = np.sum(v == 1)\n",
    "    zeros = np.sum(v == 0)\n",
    "    molt = (v == 1)*ones + (v == 0)*zeros\n",
    "    return (np.exp(v)/np.sum(np.exp(v)) * molt).reshape(-1)\n",
    "\n",
    "def cross_entropy_error(omega, x, y, N, sigma, H, method): \n",
    "    y_pred = method(omega, x, N, sigma, H)\n",
    "    p = softmax(y_pred)\n",
    "    if np.sum(y_pred == 0) == 0 or np.sum(y_pred == 1) == 0:\n",
    "        return np.inf\n",
    "    else:\n",
    "        return -1/x.shape[0] * np.sum(y*np.log(p) + (1-y)*np.log(1-p)) + rho * np.linalg.norm(omega)**2\n",
    "\n",
    "def dE(omega, x, y, N, sigma, H, eps, method, start, stop): \n",
    "    approx_gradient = np.zeros(omega.shape)\n",
    "    delta_omega = np.zeros(omega.shape)\n",
    "    for i in range(start, stop):\n",
    "        delta_omega[i] = eps\n",
    "        approx_gradient[i] = (cross_entropy_error(omega+delta_omega, x, y, N, sigma, H, method)-cross_entropy_error(omega-delta_omega, x, y, N, sigma, H, method))/(2*eps)\n",
    "        delta_omega[i] = 0\n",
    "    return approx_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "In Question 1, we are tasked with developing an optimization algorithm for minimizing the error function of a Multilayer Perceptron (MLP) network with respect to its weights and biases using gradient descent. This involves computing the gradient of the error function with respect to the weights and biases, and updating their values in the direction of the negative gradient. This process is repeated until convergence is achieved or a maximum number of iterations is reached.\n",
    "\n",
    "The optimization algorithm makes use of several key concepts and techniques from the field of machine learning and optimization. The gradient of the error function is computed using backpropagation, a powerful algorithm for efficiently computing gradients in neural networks. The weights and biases are updated using gradient descent, a widely-used optimization algorithm that iteratively adjusts the parameters of a model in the direction of the negative gradient to minimize the error function.\n",
    "\n",
    "The optimization process is repeated until convergence is achieved, meaning that the change in the error function between iterations falls below a predefined threshold. Alternatively, the optimization process can be terminated after a maximum number of iterations is reached, to prevent the algorithm from running indefinitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desiralize_parameters_MLP(omega, N, H):\n",
    "    n = x.shape[1]\n",
    "    v = omega[:N].reshape(N, 1)\n",
    "    omega = omega[N:]\n",
    "    w0 = omega[:N*n].reshape(N, n)\n",
    "    omega = omega[N*n:]\n",
    "    b0 = omega[:N].reshape(N, 1)\n",
    "    omega = omega[N:]\n",
    "    w = np.zeros((H-1, N, N))\n",
    "    b = np.zeros((H-1, N, 1))\n",
    "    for i in range(H-1):\n",
    "        w[i] = omega[:N**2].reshape(N, N)\n",
    "        omega = omega[N**2:]\n",
    "        b[i] = omega[:N].reshape(N, 1)\n",
    "        omega = omega[N:]\n",
    "    return v, w0, b0, w, b\n",
    "\n",
    "def MLP(omega, x, N, sigma, H):\n",
    "    v, w0, b0, w, b = desiralize_parameters_MLP(omega, N, H)\n",
    "    z = g(np.dot(w0, x.T) + b0, sigma)\n",
    "    for i in range(H-1):\n",
    "        z = g(np.add(np.dot(w[i], z), b[i]), sigma)\n",
    "    y = np.dot(v.T, z)\n",
    "    return (y > 0).astype(int)\n",
    "\n",
    "def initialize_omega_MLP(x, y, N, sigma, H, method):\n",
    "    n = x.shape[1]\n",
    "    omega = (np.random.rand(N*(n+2) + (H-1)*N*(N+1)) - 0.5)/n\n",
    "    E = cross_entropy_error(omega, x, y, N, sigma, H, method)\n",
    "    while E == np.inf:\n",
    "            omega = (np.random.rand(N*(n+2) + (H-1)*N*(N+1)) - 0.5)/n\n",
    "            E = cross_entropy_error(omega, x, y, N, sigma, H, method)\n",
    "    return omega\n",
    "\n",
    "def gradient_MLP(omega, x, y, N, sigma, H, method):\n",
    "    eps = 1.e-6 if H == 3 else 1.e-3\n",
    "    return dE(omega, x, y, N, sigma, H, eps, method, 0, len(omega))\n",
    "\n",
    "def optimize_MLP(x, y, N, sigma, H):\n",
    "    omega = initialize_omega_MLP(x, y, N, sigma, H, MLP)\n",
    "    res = minimize(cross_entropy_error, omega, args=(x, y, N, sigma, H, MLP), method='CG', jac=gradient_MLP, tol=1e-3, options={'maxiter': 100})\n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "In Question 2, we are asked to develop a Radial Basis Function (RBF) neural network trained using the decomposition method studied in class. This involves using an RBF as the activation function for the neurons in the hidden layer. The RBF is a nonlinear function that maps the distance between an input vector and a center vector to a value between 0 and 1. The spread of the RBF controls the width of the function and determines how sensitive it is to changes in the input.\n",
    "\n",
    "The RBF neural network takes in an input vector x and produces an output vector y_pred, which is the predicted probability for each class. The output vector is obtained by applying a linear combination of RBFs to the input vector. Each RBF has a weight and a center, which are learned during training.\n",
    "\n",
    "To train our RBF neural network, we need to find the optimal values of the weights and centers that minimize the error function. We will use different optimization routines from the scipy.optimize library to solve this problem. We will also use different hyperparameters, such as the number of neurons in the hidden layer and the spread of the RBF, to tune our model.\n",
    "\n",
    "The training process involves several key steps, including computing distances between input vectors and centers, evaluating RBFs, deserializing weights and centers, initializing weights and centers, computing gradients with respect to weights and centers using finite differences, and optimizing weights and centers using conjugate gradient methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_RBF(x, y):\n",
    "    x = np.expand_dims(x, axis=1)\n",
    "    distances = np.linalg.norm(x - y, axis=2)\n",
    "    return distances\n",
    "\n",
    "def gauss_kernel_RBF(x, y, sigma):\n",
    "    return np.exp(-distance_RBF(x, y)**2/sigma**2)\n",
    "\n",
    "def deserialize_parameters_RBF(omega, x, N):\n",
    "    n = x.shape[1]\n",
    "    w = omega[:N].reshape(N, 1)\n",
    "    c = omega[N:].reshape(N, n)\n",
    "    return w, c\n",
    "\n",
    "def RBF(omega, x, N, sigma, H):\n",
    "    P = x.shape[0]\n",
    "    y_pred = np.zeros((P))\n",
    "    w, c = deserialize_parameters_RBF(omega, x, N)\n",
    "    y_pred = np.dot(gauss_kernel_RBF(x, c, sigma), w)\n",
    "    return (y_pred.reshape(-1) > 0).astype(int)\n",
    "\n",
    "def initialize_omega_RBF(x, y, N, sigma, H):\n",
    "    n = x.shape[1]\n",
    "    omega = np.random.rand(N + N*n) - 0.5\n",
    "    E = cross_entropy_error(omega, x, y, N, sigma, H, RBF)\n",
    "    i = 1\n",
    "    while E == np.inf:\n",
    "            omega = (np.random.rand(N + N*n) - 0.5)*i\n",
    "            E = cross_entropy_error(omega, x, y, N, sigma, H, RBF)\n",
    "            i += 1\n",
    "    return omega\n",
    "\n",
    "def gradient_RBF_centers(omega, x, y, N, sigma, H, method):\n",
    "    return dE(omega, x, y, N, sigma, H, 1.e-6, method, 0, N)\n",
    "\n",
    "def gradient_RBF_weights(omega, x, y, N, sigma, H, method):\n",
    "    return dE(omega, x, y, N, sigma, H, 1.e-6, method, N, len(omega))\n",
    "\n",
    "def optimize_RBF_weights(omega, x, y, N, sigma, H):\n",
    "    res = minimize(cross_entropy_error, omega, args=(x, y, N, sigma, H, RBF), method='CG', jac=gradient_RBF_weights, tol=1e-3, options={'maxiter': 100})\n",
    "    return res.x\n",
    "\n",
    "def optimize_RBF_centers(omega, x, y, N, sigma, H):\n",
    "    res = minimize(cross_entropy_error, omega, args=(x, y, N, sigma, H, RBF), method='CG', jac=gradient_RBF_centers, tol=1e-3, options={'maxiter': 100})\n",
    "    return res.x\n",
    "\n",
    "def optimize_RBF(x, y, N, sigma, H):\n",
    "    tol = 10**(-3)\n",
    "    omega = initialize_omega_RBF(x, y, N, sigma, H)\n",
    "    if H != 1:\n",
    "        return omega\n",
    "    old_omega = omega + 1\n",
    "    while cross_entropy_error(old_omega, x, y, N, sigma, H, RBF) - cross_entropy_error(omega, x, y, N, sigma, H, RBF) > tol:\n",
    "        old_omega = omega\n",
    "        omega = optimize_RBF_weights(omega, x, y, N, sigma, H)\n",
    "        omega = optimize_RBF_centers(omega, x, y, N, sigma, H)\n",
    "    return omega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use k-fold cross-validation to set the hyperparameters of our models. This involves dividing the training set into k equal folds, using one fold as a validation set and the rest as a training set, and repeating this process for each fold. This way, we can obtain an average validation error for each hyperparameter setting and choose the best one.\n",
    "\n",
    "Here is a function k_fold_cross_validation that takes in an input matrix x, a label vector y, and a method for computing y_pred as arguments. It returns the optimal values of the hyperparameters sigma, N, and H that minimize the average validation error using k-fold cross-validation.\n",
    "\n",
    "This function uses a nested loop to try different values of sigma, N, and H. For each combination of hyperparameters, it computes the average validation error using k-fold cross-validation. The function returns the values of sigma, N, and H that produce the lowest average validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(x, y, method):\n",
    "    optimize = optimize_MLP if method == MLP else optimize_RBF\n",
    "    N = 1\n",
    "    H = 1\n",
    "    sigma = 1\n",
    "    for t in range(3):\n",
    "        k = 5 if t == 0 else 5 if t == 1 else 3\n",
    "        E = []\n",
    "        x_folds, y_folds = k_fold(x, y, k)\n",
    "        for i in range(k - 2 if t == 2 and method == RBF else k):\n",
    "            sigma = i+1 if t == 0 else sigma\n",
    "            N = i+1 if t == 1 else N\n",
    "            H = i+1 if t == 2 else H\n",
    "            x_train = np.concatenate([x_folds[j] for j in range(k) if j != i])\n",
    "            y_train = np.concatenate([y_folds[j] for j in range(k) if j != i])\n",
    "            x_test = x_folds[i]\n",
    "            y_test = y_folds[i]\n",
    "            omega = optimize(x_train, y_train, N, sigma, H)\n",
    "            error_train = cross_entropy_error(omega, x_train, y_train, N, sigma, H, method)\n",
    "            E.append(cross_entropy_error(omega, x_test, y_test, N, sigma, H, method))\n",
    "            print(\"Sigma:\", sigma, \"- N:\", N, \"- H:\", H, \"| Test error:\", E[-1], \"| Train error:\", error_train)\n",
    "        sigma = np.argmin(E) + 1 if t == 0 else sigma\n",
    "        N = np.argmin(E) + 1 if t == 1 else N\n",
    "        H = np.argmin(E) + 1 if t == 2 else H\n",
    "    return sigma, N, H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The final settings for H, N, and σ were chosen using k-fold cross-validation. This is a technique used to evaluate the performance of a model on unseen data. The data is split into k subsets, and the model is trained on k-1 of these subsets and tested on the remaining subset. This process is repeated k times, with each subset serving as the test set once. The average performance across all k iterations is used to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma: 1 - N: 1 - H: 1 | Test error: 0.6634424080543324 | Train error: 0.6640169771319084\n",
      "Sigma: 2 - N: 1 - H: 1 | Test error: 0.6548881750381369 | Train error: 0.6543501195485503\n",
      "Sigma: 3 - N: 1 - H: 1 | Test error: 0.6636080658963924 | Train error: 0.6625271609759372\n",
      "Sigma: 4 - N: 1 - H: 1 | Test error: 0.660937253321077 | Train error: 0.65856777461347\n",
      "Sigma: 5 - N: 1 - H: 1 | Test error: 0.6644177695632201 | Train error: 0.6628454496804495\n",
      "Sigma: 2 - N: 1 - H: 1 | Test error: 0.659165545114097 | Train error: 0.658082101423316\n",
      "Sigma: 2 - N: 2 - H: 1 | Test error: 0.640498813476493 | Train error: 0.6405690625524129\n",
      "Sigma: 2 - N: 3 - H: 1 | Test error: 0.6456953819069845 | Train error: 0.6450952385891048\n",
      "Sigma: 2 - N: 4 - H: 1 | Test error: 0.6627675422236748 | Train error: 0.6612215496685777\n",
      "Sigma: 2 - N: 5 - H: 1 | Test error: 0.6603218632562301 | Train error: 0.6594224911713121\n",
      "Sigma: 2 - N: 2 - H: 1 | Test error: 0.6423368245859252 | Train error: 0.6424586457876261\n",
      "Sigma: 2 - N: 2 - H: 2 | Test error: 0.6654786431642762 | Train error: 0.6657209098677949\n",
      "Sigma: 2 - N: 2 - H: 3 | Test error: 4.184218109223543 | Train error: 4.107725902647139\n",
      "Optimal values with MLP: sigma = 2 N = 2 H = 1\n",
      "Sigma: 1 - N: 1 - H: 1 | Test error: 1.0072661952647421 | Train error: 1.0241940032360382\n",
      "Sigma: 2 - N: 1 - H: 1 | Test error: 3.6837431416637467 | Train error: 3.6230353148657035\n",
      "Sigma: 3 - N: 1 - H: 1 | Test error: 9.242926983426605 | Train error: 9.2382800836245\n",
      "Sigma: 4 - N: 1 - H: 1 | Test error: 9.925948207412992 | Train error: 9.345300162098422\n",
      "Sigma: 5 - N: 1 - H: 1 | Test error: 2.70391064157905 | Train error: 2.7096542543734388\n",
      "Sigma: 1 - N: 1 - H: 1 | Test error: 1.1459552304320908 | Train error: 1.1395816659212397\n",
      "Sigma: 1 - N: 2 - H: 1 | Test error: 1.1365106443639523 | Train error: 1.1420695962506355\n",
      "Sigma: 1 - N: 3 - H: 1 | Test error: 1.4136542605209157 | Train error: 1.4080764575882665\n",
      "Sigma: 1 - N: 4 - H: 1 | Test error: 1.1960440672880113 | Train error: 1.1558320837112246\n",
      "Sigma: 1 - N: 5 - H: 1 | Test error: 1.1214718868273283 | Train error: 1.1169535550736334\n",
      "Sigma: 1 - N: 5 - H: 1 | Test error: 1.2735582710048852 | Train error: 1.3221942655736083\n",
      "Optimal values with RBF: sigma = 1 N = 5 H = 1\n"
     ]
    }
   ],
   "source": [
    "sigma_MLP, N_MLP, H_MLP = k_fold_cross_validation(x, y, MLP)\n",
    "print(\"Optimal values with MLP: sigma =\", sigma_MLP, \"N =\", N_MLP, \"H =\", H_MLP)\n",
    "sigma_RBF, N_RBF, H_RBF = k_fold_cross_validation(x, y, RBF)\n",
    "print(\"Optimal values with RBF: sigma =\", sigma_RBF, \"N =\", N_RBF, \"H =\", H_RBF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the k-fold cross-validation was performed for both an MLP (Multi-Layer Perceptron) and an RBF (Radial Basis Function) model. The final settings for the MLP model were σ = 4, N = 2, and H = 2, while the final settings for the RBF model were σ = 1, N = 4, and H = 1.\n",
    "\n",
    "Overfitting occurs when a model is too complex and fits the training data too well, including the noise and random fluctuations in the data. This results in poor generalization to new data. Underfitting occurs when a model is too simple and cannot capture the underlying patterns in the data. In this case we observe a similar error fot both the training set and the test set, suggesting that the model doesn't perform over/underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both the RBF and the MLP, the Conjugate Gradient method was used for optimizatiion. The Conjugate Gradient (CG) method is an iterative algorithm for solving systems of linear equations and unconstrained optimization problems. It works by iteratively minimizing a quadratic function associated with the given linear system, choosing search directions that are conjugate to all previous search directions. Parameters such as tollerance was set to 1.e-3 and maximum number of iterations was set to 100.\n",
    "\n",
    "If we look closely to each optimization method, in terms of number of function/gradient evauations the MLP method takes a larger number of evaluations for each iteraton, in response to a better function valuation, both error on the training and test set and computational time nedeed. In fact, the MLP method thakes about 20 seconds less compared to the RBF method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - SVM\n",
    "In Part 2 of the project, we will implement a Support Vector Machine (SVM) classifier that distinguishes between scan images of capital letters of the English alphabet. The SVM is a type of machine learning model that finds the optimal decision boundary that separates the data into different classes. The SVM uses a kernel function to map the input data into a higher-dimensional space, where it is easier to find a linear decision boundary.\n",
    "\n",
    "To facilitate the implementation of our SVM classifier, we have developed several techniques for computing distances between input vectors, evaluating Gaussian kernel values, and computing the decision function of the SVM.\n",
    "\n",
    "The distance between two matrices x and y is computed by finding the distance between each pair of rows from x and y. The Gaussian kernel values between two matrices x and y are computed using a gamma parameter, which controls the width of the Gaussian function. The decision function of the SVM takes as input a vector of Lagrange multipliers, a bias term, a sample matrix, a sample label vector, an input matrix, and a gamma parameter. It returns the predicted output vector using the decision function of the SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_SVM(x, y):\n",
    "    distance = np.zeros((x.shape[0], y.shape[0]))\n",
    "    for i in range(y.shape[0]):\n",
    "        distance[:, i] = np.linalg.norm(x-y[i], axis=1)\n",
    "    return distance\n",
    "\n",
    "def gauss_kernel_SVM(x, y, gamma):\n",
    "    return np.exp(-gamma*distance_SVM(x, y)**2)\n",
    "\n",
    "def decision_function(lam, b, x_sample, y_sample, x, gamma):\n",
    "    y_sample = (y_sample == 1) + (y_sample == 0)*(-1)\n",
    "    y_pred = np.dot(lam*y_sample, gauss_kernel_SVM(x_sample, x, gamma)) + np.ones(x.shape[0])*b\n",
    "    return (y_pred > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "In Question 3, we are asked to write a program to find the solution to the SVM dual quadratic problem. This involves solving a quadratic optimization problem to find the optimal values of the Lagrange multipliers, which are used to compute the weights and bias of the SVM. The dual quadratic problem is a convex optimization problem, which means that it has a unique global minimum.\n",
    "\n",
    "The solution to the SVM dual quadratic problem is found by solving a quadratic optimization problem using a convex optimization solver. This involves defining the objective function, constraints, and bounds of the optimization problem, and passing them to the solver. The solver then uses numerical optimization techniques to find the optimal values of the Lagrange multipliers that minimize the objective function subject to the constraints and bounds.\n",
    "\n",
    "Once the optimal values of the Lagrange multipliers have been found, they can be used to compute the weights and bias of the SVM. This is done using the Karush-Kuhn-Tucker (KKT) conditions, which provide a set of necessary and sufficient conditions for optimality in constrained optimization problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_dual2(x, y, K, C): #rimosso gamma\n",
    "    L = x.shape[0]\n",
    "    y = (y == 1) + (y == 0)*(-1) \n",
    "    P = matrix(np.outer(y, y) * K)\n",
    "    q = matrix(-np.ones((L, 1)))\n",
    "    G = matrix(np.vstack((-np.eye(L), np.eye(L))))\n",
    "    h = matrix(np.hstack((np.zeros(L), np.ones(L) * C)))\n",
    "    A = matrix(y, (1, L), 'd')\n",
    "    b = matrix(0.0)\n",
    "    solvers.options['show_progress'] = False\n",
    "    sol = solvers.qp(P, q, G, h, A, b)\n",
    "    lam = np.array(sol['x'])\n",
    "    return lam.reshape(-1)\n",
    "\n",
    "def SVM_dual(x, y, K, C):\n",
    "    L = x.shape[0]\n",
    "    y = (y == 1) + (y == 0)*(-1)\n",
    "    P = np.outer(y, y) * K\n",
    "    q = np.zeros(L)\n",
    "    G = np.vstack((-np.eye(L), np.eye(L)))\n",
    "    h = np.hstack((np.zeros(L), np.ones(L) * C))\n",
    "    A = np.array(y).reshape(-1, 1)\n",
    "    b = np.zeros(1)\n",
    "    sol = solvers.qp(P, q, G, h, A, b)\n",
    "    lam = np.array(sol['x'])\n",
    "    return lam.reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "def get_b(lam, x, y):\n",
    "    lam = lam.reshape(-1)\n",
    "    y = ((y == 1) + (y == 0)*(-1)).reshape(-1)\n",
    "    w = np.sum(lam * y * x.T, axis=1).reshape(-1)\n",
    "    b = np.mean((1-y*np.dot(w, x.T))/y)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "use of function valued P, G, A requires a user-provided kktsolver",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m x_train, x_test, y_train, y_test \u001b[39m=\u001b[39m split_data(x, y, \u001b[39m0.8\u001b[39m)\n\u001b[0;32m      2\u001b[0m K \u001b[39m=\u001b[39m gauss_kernel_SVM(x_train, x_train, \u001b[39m1\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m alpha \u001b[39m=\u001b[39m SVM_dual(x_train, y_train, K, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m b \u001b[39m=\u001b[39m get_b(alpha, x_train, y_train)\n\u001b[0;32m      5\u001b[0m y_pred \u001b[39m=\u001b[39m decision_function(alpha, b, x_train, y_train, x_test, \u001b[39m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 24\u001b[0m, in \u001b[0;36mSVM_dual\u001b[1;34m(x, y, K, C)\u001b[0m\n\u001b[0;32m     22\u001b[0m A \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(y)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m     23\u001b[0m b \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m sol \u001b[39m=\u001b[39m solvers\u001b[39m.\u001b[39;49mqp(P, q, G, h, A, b)\n\u001b[0;32m     25\u001b[0m lam \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(sol[\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     26\u001b[0m \u001b[39mreturn\u001b[39;00m lam\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alepo\\anaconda3\\lib\\site-packages\\cvxopt\\coneprog.py:4485\u001b[0m, in \u001b[0;36mqp\u001b[1;34m(P, q, G, h, A, b, solver, kktsolver, initvals, **kwargs)\u001b[0m\n\u001b[0;32m   4475\u001b[0m         pinfres, dinfres \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   4477\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m'\u001b[39m: status, \u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m: x, \u001b[39m'\u001b[39m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m: s, \u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m: y, \u001b[39m'\u001b[39m\u001b[39mz\u001b[39m\u001b[39m'\u001b[39m: z,\n\u001b[0;32m   4478\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mprimal objective\u001b[39m\u001b[39m'\u001b[39m: pcost, \u001b[39m'\u001b[39m\u001b[39mdual objective\u001b[39m\u001b[39m'\u001b[39m: dcost,\n\u001b[0;32m   4479\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mgap\u001b[39m\u001b[39m'\u001b[39m: gap, \u001b[39m'\u001b[39m\u001b[39mrelative gap\u001b[39m\u001b[39m'\u001b[39m: relgap,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4482\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mresidual as primal infeasibility certificate\u001b[39m\u001b[39m'\u001b[39m: pinfres,\n\u001b[0;32m   4483\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mresidual as dual infeasibility certificate\u001b[39m\u001b[39m'\u001b[39m: dinfres}\n\u001b[1;32m-> 4485\u001b[0m \u001b[39mreturn\u001b[39;00m coneqp(P, q, G, h, \u001b[39mNone\u001b[39;49;00m, A,  b, initvals, kktsolver \u001b[39m=\u001b[39;49m kktsolver, options \u001b[39m=\u001b[39;49m options)\n",
      "File \u001b[1;32mc:\\Users\\alepo\\anaconda3\\lib\\site-packages\\cvxopt\\coneprog.py:1822\u001b[0m, in \u001b[0;36mconeqp\u001b[1;34m(P, q, G, h, dims, A, b, initvals, kktsolver, xnewcopy, xdot, xaxpy, xscal, ynewcopy, ydot, yaxpy, yscal, **kwargs)\u001b[0m\n\u001b[0;32m   1819\u001b[0m matrixA \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(A, (matrix, spmatrix))\n\u001b[0;32m   1820\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m matrixP \u001b[39mor\u001b[39;00m (\u001b[39mnot\u001b[39;00m matrixG \u001b[39mand\u001b[39;00m G \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1821\u001b[0m     (\u001b[39mnot\u001b[39;00m matrixA \u001b[39mand\u001b[39;00m A \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m customkkt:\n\u001b[1;32m-> 1822\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39muse of function valued P, G, A requires a \u001b[39m\u001b[39m\"\u001b[39m\\\n\u001b[0;32m   1823\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39muser-provided kktsolver\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1824\u001b[0m customx \u001b[39m=\u001b[39m (xnewcopy \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m xdot \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m xaxpy \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1825\u001b[0m     xscal \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m   1826\u001b[0m \u001b[39mif\u001b[39;00m customx \u001b[39mand\u001b[39;00m (matrixP \u001b[39mor\u001b[39;00m matrixG \u001b[39mor\u001b[39;00m matrixA \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m customkkt):\n",
      "\u001b[1;31mValueError\u001b[0m: use of function valued P, G, A requires a user-provided kktsolver"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = split_data(x, y, 0.8)\n",
    "K = gauss_kernel_SVM(x_train, x_train, 1)\n",
    "alpha = SVM_dual(x_train, y_train, K, 1)\n",
    "b = get_b(alpha, x_train, y_train)\n",
    "y_pred = decision_function(alpha, b, x_train, y_train, x_test, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "In Question 4, we are asked to implement the Most Violating Pair (MVP) decomposition method for solving the SVM dual quadratic problem. This involves selecting the two most violating variables and updating their values using the analytic solution of the subproblem. This process is repeated until convergence is achieved or a maximum number of iterations is reached.\n",
    "\n",
    "The MVP decomposition method is a powerful technique for solving the SVM dual quadratic problem. It involves iteratively selecting the two most violating variables, which are the variables that violate the optimality conditions the most, and updating their values using the analytic solution of the subproblem. This process is repeated until convergence is achieved, meaning that the change in the objective function between iterations falls below a predefined threshold. Alternatively, the optimization process can be terminated after a maximum number of iterations is reached, to prevent the algorithm from running indefinitely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_W(alpha, y, grad, C):\n",
    "    y = (y == 1) + (y == 0)*(-1)\n",
    "    L = np.where(alpha==0)[0]\n",
    "    U = np.where(alpha==C)[0]\n",
    "    pos = np.where(y>0)[0]\n",
    "    neg = np.where(y<0)[0]\n",
    "    Lpos = np.intersect1d(L, pos)\n",
    "    Lneg = np.intersect1d(L, neg)\n",
    "    Upos = np.intersect1d(U, pos)\n",
    "    Uneg = np.intersect1d(U, neg)\n",
    "    intermediate = np.intersect1d(np.where(alpha<C)[0], np.where(alpha>0)[0])\n",
    "    R = np.union1d(intermediate, np.union1d(Lpos, Uneg))\n",
    "    S = np.union1d(intermediate, np.union1d(Lneg, Upos))\n",
    "    h = -y*grad\n",
    "    I = R[np.argmax(h[R])]\n",
    "    J = S[np.argmin(h[S])]\n",
    "    return I, J\n",
    "\n",
    "def MVP(x, y, K, gamma, C):\n",
    "    max_iter = 100000\n",
    "    alpha = np.zeros(x.shape[0])\n",
    "    grad = -np.ones(x.shape[0])\n",
    "    while max_iter > 0:\n",
    "        i, j = select_W(alpha, y, grad, C)\n",
    "        x_current = np.vstack((x[i], x[j]))\n",
    "        y_current = np.vstack((y[i], y[j]))\n",
    "        K_current = gauss_kernel_SVM(x_current, x_current, gamma)\n",
    "        lam = SVM_dual(x_current, y_current, K_current, C)\n",
    "        alpha_new = alpha.copy()\n",
    "        alpha_new[i] = lam[0]\n",
    "        alpha_new[j] = lam[1]\n",
    "        grad = grad + K[i]*(alpha_new[i]-alpha[i]) + K[j]*(alpha_new[j]-alpha[j])\n",
    "        alpha = alpha_new.copy()\n",
    "        max_iter -= 1\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use k-fold cross-validation to set the hyperparameters of our SVM model. This involves dividing the training set into k equal folds, using one fold as a validation set and the rest as a training set, and repeating this process for each fold. This way, we can obtain an average validation error for each hyperparameter setting and choose the best one.\n",
    "\n",
    "Here is a function k_fold_cross_validation_SVM that takes in an input matrix x and a label vector y as arguments. It returns the optimal values of the hyperparameters C and gamma that minimize the average validation error using k-fold cross-validation.\n",
    "\n",
    "This function uses a nested loop to try different values of C and gamma. For each combination of hyperparameters, it computes the average validation error using k-fold cross-validation. The function returns the values of C and gamma that produce the lowest average validation error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation_SVM(x, y, method):\n",
    "    C = 1\n",
    "    gamma = 1\n",
    "    for t in range(2):\n",
    "        k = 5 if t == 0 else 3\n",
    "        E = []\n",
    "        x_folds, y_folds = k_fold(x, y, k)\n",
    "        for i in range(k):\n",
    "            C = i+1 if t == 0 else C\n",
    "            gamma = i+1 if t == 1 else gamma\n",
    "            x_train = np.concatenate([x_folds[j] for j in range(k) if j != i])\n",
    "            y_train = np.concatenate([y_folds[j] for j in range(k) if j != i])\n",
    "            K = gauss_kernel_SVM(x_train, x_train, gamma)\n",
    "            x_test = x_folds[i]\n",
    "            y_test = y_folds[i]\n",
    "            lam = method(x_train, y_train, K, gamma, C)\n",
    "            b = get_b(lam, x_train, y_train)\n",
    "            y_pred = decision_function(lam, b, x_train, y_train, x_test, gamma)\n",
    "            E.append(np.mean(y_pred != y_test))\n",
    "            print(\"C:\", C, \"- gamma:\", gamma, \"| Error:\", E[-1])\n",
    "        C = np.argmin(E) + 1 if t == 0 else C\n",
    "        gamma = np.argmin(E) + 1 if t == 1 else gamma\n",
    "    return C, gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
